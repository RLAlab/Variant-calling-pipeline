{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole genome sequencing analysis workflow\n",
    "\n",
    "### Problem\n",
    "\n",
    "Adaptive laboratory evolution was conducted on a S. cerevisiae co-culture of two cross-feeding strains (sHP063, sHP067), generating the evolved cross-feeding strains B2 (evolved sHP063) and R8 (evolved sHP067).\n",
    "We want to understand what adaptation has the evolution process resulted in at a genomic level using the WGS data by comparing the evolved R8 and B2 strains against their non-evolved parentals (sHP067 and sHP063, respectively).\n",
    " \n",
    "Samples are labelled as follows:\n",
    "- BY4741, base strain (sHP063 and sHP067 were built from this “reference genome”)\n",
    "- sHP063 (non-evolved)\n",
    "- sHP067 (non-evolved)\n",
    "- B2 (sHP063 evolved)\n",
    "- R8 (sHP067 evolved)\n",
    "\n",
    "![co-culture](co-culture.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequencing was performed on the Illumina platform, generating raw reads (or raw data) stored in FASTQ (.fq) format files. These files contain the sequencing reads along with base quality scores. Raw reads were filtered to produce clean reads, with the following criteria applied:\n",
    "\n",
    "1. Paired reads are removed if either read contains adapter contamination.\n",
    "2. Paired reads are removed if uncertain nucleotides (represented by \"N\") make up more than 10% of either read.\n",
    "3. Paired reads are removed if low-quality nucleotides (base quality ≤ 5) constitute more than 50% of either read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: De novo genome assembly of ancestral strains\n",
    "\n",
    "To identify genetic variations in the evolved strains relative to their ancestral, non-evolved parent strains, the first step is to reconstruct the reference FASTA sequence for each ancestral strain. This is achieved through de novo genome assembly, which builds a complete genome sequence from raw sequencing reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and activate environment\n",
    "conda install -c conda-forge mamba --yes\n",
    "\n",
    "mamba create -n de_novo_assembly -c bioconda -c conda-forge fastqc spades megahit quast busco bowtie2 ragtag --yes\n",
    "mamba create -n ragout_env -c bioconda -c conda-forge ragout --yes   # uses incompatible package specs with programs above\n",
    "conda activate de_novo_assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess sequence data quality\n",
    "cd X204SC24082098-Z01-F002_01/01.RawData/sHP063/\n",
    "fastqc sHP063_1.fq.gz sHP063_2.fq.gz -t 4\n",
    "\n",
    "cd X204SC24082098-Z01-F002_01/01.RawData/sHP067/\n",
    "fastqc sHP067_1.fq.gz sHP067_2.fq.gz -t 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall the reads quality is good (as they had already been filtered by Novogene) so no need for trimming.\n",
    "\n",
    "The next step is assembling the reads. I tested different assembly programs ([SPAdes](https://ablab.github.io/spades/) and [MEGAHIT](https://github.com/voutcn/megahit/wiki)) with different parameters and then compared the results. \n",
    "\n",
    "First I run an error-correction step with SPAdes, which tends to improve assembly results according to [this tutorial](https://astrobiomike.github.io/genomics/de_novo_assembly) I am following. This is the most computationally intensive step of the workflow (took up to 4h on the HPC as a submitted job)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd X204SC24082098-Z01-F002_01/01.RawData/sHP063/\n",
    "spades.py -1 sHP063_1.fq.gz -2 sHP063_2.fq.gz -o spades_error_corrected_reads -t 50 -m 500 --only-error-correction\n",
    "\n",
    "cd X204SC24082098-Z01-F002_01/01.RawData/sHP067/\n",
    "spades.py -1 sHP067_1.fq.gz -2 sHP067_2.fq.gz -o spades_error_corrected_reads -t 50 -m 500 --only-error-correction\n",
    "\n",
    "# -t : number of threads\n",
    "# -m : RAM limit in Gb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if you want to submit it as a job to the HPC - so that it runs in the background without the need for your laptop to stay on - you can create a file (sHP063_errorcorrection) containing the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PBS -lwalltime=32:00:00\n",
    "#PBS -lselect=1:ncpus=20:mem=100gb\n",
    "\n",
    "eval \"$(~/anaconda3/bin/conda shell.bash hook)\"\n",
    "\n",
    "conda activate de_novo_assembly\n",
    "\n",
    "cd /rds/general/user/gd1122/home/Diego_WGS/X204SC24082098-Z01-F002_01/01.RawData/sHP063\n",
    "spades.py -1 sHP063_1.fq.gz -2 sHP063_2.fq.gz -o spades_error_corrected_reads -t 50 -m 500 --only-error-correction\n",
    "\n",
    "conda deactivate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and you can submit it with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qsub sHP063_errorcorrection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And do the same for sHP067 in parallel. Full guide on the Imperial HPC and submit jobs guidance [here](https://icl-rcs-user-guide.readthedocs.io/en/latest/hpc/).\n",
    "\n",
    "The next step is running the actual assembly on the error-corrected reads. Once more, this step can be computationally intensive so it is recommended to run it as a submitted job as above. For sHP063, I tried the following parameters for SPAdes (guide for command line options [here](https://ablab.github.io/spades/running.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spades.py -1 spades_error_corrected_reads/corrected/sHP063_1.fq00.0_0.cor.fastq.gz \\\n",
    "          -2 spades_error_corrected_reads/corrected/sHP063_2.fq00.0_0.cor.fastq.gz \\\n",
    "          -o spades_assembly_default --only-assembler\n",
    "\n",
    "spades.py -1 spades_error_corrected_reads/corrected/sHP063_1.fq00.0_0.cor.fastq.gz \\\n",
    "          -2 spades_error_corrected_reads/corrected/sHP063_2.fq00.0_0.cor.fastq.gz \\\n",
    "          -o spades_assembly_careful --only-assembler --careful\n",
    "\n",
    "spades.py -1 spades_error_corrected_reads/corrected/sHP063_1.fq00.0_0.cor.fastq.gz \\\n",
    "          -2 spades_error_corrected_reads/corrected/sHP063_2.fq00.0_0.cor.fastq.gz \\\n",
    "          -o spades_assembly_careful_kmers --only-assembler --careful -k 21,33,55,77\n",
    "\n",
    "spades.py -1 spades_error_corrected_reads/corrected/sHP063_1.fq00.0_0.cor.fastq.gz \\\n",
    "          -2 spades_error_corrected_reads/corrected/sHP063_2.fq00.0_0.cor.fastq.gz \\\n",
    "          -o spades_assembly_isolate --only-assembler --isolate\n",
    "\n",
    "spades.py -1 spades_error_corrected_reads/corrected/sHP063_1.fq00.0_0.cor.fastq.gz \\\n",
    "          -2 spades_error_corrected_reads/corrected/sHP063_2.fq00.0_0.cor.fastq.gz \\\n",
    "          -o spades_assembly_isolate_kmers --only-assembler --isolate -k 21,33,55,77"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and for MEGAHIT (guide [here](https://github.com/voutcn/megahit/wiki/Assembly-Tips)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "megahit -1 spades_error_corrected_reads/corrected/sHP063_1.fq00.0_0.cor.fastq.gz \\\n",
    "        -2 spades_error_corrected_reads/corrected/sHP063_2.fq00.0_0.cor.fastq.gz \\\n",
    "        -o megahit_assembly\n",
    "\n",
    "megahit -1 spades_error_corrected_reads/corrected/sHP063_1.fq00.0_0.cor.fastq.gz \\\n",
    "        -2 spades_error_corrected_reads/corrected/sHP063_2.fq00.0_0.cor.fastq.gz \\\n",
    "        -o megahit_assembly_mincount --min-count 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And did the same for sHP067.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare our results using two different programs, [QUAST](https://quast.sourceforge.net/docs/manual.html) and [BUSCO](https://busco.ezlab.org/busco_userguide.html). Quast is a tool to evaluate genome assemblies by computing various metrics and to compare genome assembly with a reference genome while BUSCO allows a measure for quantitative assessment of genome assembly based on evolutionarily informed expectations of gene content.\n",
    "\n",
    "First we need to download a reference genome to use as comparison from NCBI. We can do this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/146/045/GCF_000146045.2_R64/GCF_000146045.2_R64_genomic.fna.gz\n",
    "wget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/146/045/GCF_000146045.2_R64/GCF_000146045.2_R64_genomic.gff.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can run QUAST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quast -o X204SC24082098-Z01-F002_01/01.RawData/sHP063/quastout_sHP063 -R GCF_000146045.2_R64_genomic.fna -G GCF_000146045.2_R64_genomic.gff -m 1000 \\\n",
    "-l \"spades_default, spades_careful, spades_isolate, spades_careful_kmers, spades_isolate_kmers, megahit, megahit_mincount\" \\\n",
    "X204SC24082098-Z01-F002_01/01.RawData/sHP063/spades_assembly_default/contigs.fasta \\\n",
    "X204SC24082098-Z01-F002_01/01.RawData/sHP063/spades_assembly_careful/contigs.fasta \\\n",
    "X204SC24082098-Z01-F002_01/01.RawData/sHP063/spades_assembly_isolate/contigs.fasta \\\n",
    "X204SC24082098-Z01-F002_01/01.RawData/sHP063/spades_assembly_careful_kmers/contigs.fasta \\\n",
    "X204SC24082098-Z01-F002_01/01.RawData/sHP063/spades_assembly_isolate_kmers/contigs.fasta \\\n",
    "X204SC24082098-Z01-F002_01/01.RawData/sHP063/megahit_assembly/final.contigs.fa \\\n",
    "X204SC24082098-Z01-F002_01/01.RawData/sHP063/megahit_assembly_mincount/final.contigs.fa "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From its report, we can see that overall the best assembly seems to be the one obtained with the --isolate flag, with or without the kmers options.\n",
    "To further assess it, we can also run BUSCO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "busco -i X204SC24082098-Z01-F002_01/01.RawData/sHP063/spades_assembly_isolate/contigs.fasta -m genome \\\n",
    "-o X204SC24082098-Z01-F002_01/01.RawData/sHP063/buscoout_sHP063 \\\n",
    "-l saccharomycetes_odb10 -c 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\t***** Results: *****\n",
    "\n",
    "\tC:99.1%[S:97.2%,D:1.9%],F:0.1%,M:0.7%,n:2137,E:0.1%\t   \n",
    "\t2118\tComplete BUSCOs (C)\t(of which 2 contain internal stop codons)\t\t   \n",
    "\t2077\tComplete and single-copy BUSCOs (S)\t   \n",
    "\t41\tComplete and duplicated BUSCOs (D)\t   \n",
    "\t3\tFragmented BUSCOs (F)\t\t\t   \n",
    "\t16\tMissing BUSCOs (M)\t\t\t   \n",
    "\t2137\tTotal BUSCO groups searched\t\t   \n",
    "\n",
    "    Assembly Statistics:\n",
    "\t1471\tNumber of scaffolds\n",
    "\t1471\tNumber of contigs\n",
    "\t11795778\tTotal length\n",
    "\t0.000%\tPercent gaps\n",
    "\t125 KB\tScaffold N50\n",
    "\t125 KB\tContigs N50\n",
    "    \n",
    "Then do the same with sHP067."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now obtained contigs for both of parental species. We can try to improve the genome assembly by assembling and ordering contigs (contiguous sequences) into larger structures (scaffolds), hopefully representing chromosomes or entire genomes (NB: this is a hard task, especially only using short reads).\n",
    "We use two programs to achieve this, [Ragout](https://github.com/mikolmogorov/Ragout/blob/master/docs/USAGE.md) and [RagTag](https://github.com/malonge/RagTag/wiki), and then we can compare their output using QUAST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Ragout, we have to create a \"recipe file\" that describes the run configuration and parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference and target genome names (required)\n",
    ".references = scerR64\n",
    ".target = sHP063\n",
    "\n",
    "#paths to genome fasta files (required for Sibelia)\n",
    "scerR64.fasta = path/to/file/GCF_000146045.2_R64_genomic.fna\n",
    "sHP063.fasta = path/to/folder/X204SC24082098-Z01-F002_01/01.RawData/sHP063/spades_assembly_isolate/contigs.fasta\n",
    "\n",
    "#reference to use for scaffold naming (optional)\n",
    ".naming_ref = scerR64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then run it with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch environment\n",
    "conda deactivate\n",
    "conda activate ragout_env\n",
    "\n",
    "ragout X204SC24082098-Z01-F002_01/01.RawData/sHP063/recipe.txt \\\n",
    "-o X204SC24082098-Z01-F002_01/01.RawData/sHP063/ragout_sHP063 --refine -t 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We than try RagTag, with the scaffold module, first using the -C flag that concatenates unplaced contigs to make 'chr0' and then without it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch environment\n",
    "conda deactivate\n",
    "conda activate de_novo_assembly\n",
    "\n",
    "ragtag.py scaffold GCF_000146045.2_R64_genomic.fna \\\n",
    "X204SC24082098-Z01-F002_01/01.RawData/sHP063/spades_assembly_careful/contigs.fasta \\\n",
    "-o X204SC24082098-Z01-F002_01/01.RawData/sHP063/ragtag_scaffold_C_sHP063 -r -C\n",
    "\n",
    "ragtag.py scaffold GCF_000146045.2_R64_genomic.fna \\\n",
    "X204SC24082098-Z01-F002_01/01.RawData/sHP063/spades_assembly_careful/contigs.fasta \\\n",
    "-o X204SC24082098-Z01-F002_01/01.RawData/sHP063/ragtag_scaffold_sHP063 -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we run QUAST to compare the outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quast -o X204SC24082098-Z01-F002_01/01.RawData/sHP063/quastout_scaffold_sHP063 -R GCF_000146045.2_R64_genomic.fna -G GCF_000146045.2_R64_genomic.gff -m 1000 \\\n",
    "-l \"ragout, ragtag_C, ragtag\" \\\n",
    "X204SC24082098-Z01-F002_01/01.RawData/sHP063/ragout_sHP063/sHP063_scaffolds.fasta \\\n",
    "X204SC24082098-Z01-F002_01/01.RawData/sHP063/ragtag_scaffold_C_sHP063/ragtag.scaffold.fasta \\\n",
    "X204SC24082098-Z01-F002_01/01.RawData/sHP063/ragtag_scaffold_sHP063/ragtag.scaffold.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and do the same with sHP067.\n",
    "\n",
    "It looks like in both cases the best reconstructed genome assembly is the one obtained with RagTag without the -C flag. These will now become our new reference genomes to use for the next step in the pipeline, variant calling. \n",
    "\n",
    "NB. Don't forget to deactivate the conda environment once you are done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda deactivate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Variant calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step  is to identify genomic variants in the evolved strains, including single nucleotide polymorphisms (SNPs), DNA insertions and deletions (indels), copy number variants (CNVs) and structural variants (SVs). \n",
    "Let's create a new folder for this and copy the relevant files that will be used in this step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir variant_calling && cd variant_calling\n",
    "mkdir sHP063 && mkdir sHP067\n",
    "\n",
    "# copy new reference fasta for sHP063 and B2 evolved strain fastq reads\n",
    "cp ../X204SC24082098-Z01-F002_01/01.RawData/sHP063/ragtag_scaffold_sHP063/ragtag.scaffold.fasta sHP063/\n",
    "mv sHP063/ragtag.scaffold.fasta sHP063/sHP063_reference.fasta\n",
    "cp ../X204SC24082098-Z01-F002_01/01.RawData/B2/B2_*.fq.gz sHP063/\n",
    "\n",
    "# copy new reference fasta for sHP067 and R8 evolved strain fastq reads\n",
    "cp ../X204SC24082098-Z01-F002_01/01.RawData/sHP067/ragtag_scaffold_sHP067/ragtag.scaffold.fasta sHP067/\n",
    "mv sHP067/ragtag.scaffold.fasta sHP067/sHP067_reference.fasta\n",
    "cp ../X204SC24082098-Z01-F002_01/01.RawData/R8/R8_*.fq.gz sHP067/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 SNPs and indels\n",
    "\n",
    "To identify SNPs and indels I am following [this tutorial](https://gencore.bio.nyu.edu/variant-calling-pipeline-gatk4/), which also provides an automated script to run all the steps using Nextflow on the HPC.\n",
    "\n",
    "![variantcalling_GATK](Variant-Calling-Pipeline-GATK4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first set up the conda environment and nextflow, as described in the tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git clone https://github.com/gencorefacility/variant-calling-pipeline-gatk4\n",
    "    \n",
    "conda config --add channels defaults\n",
    "conda config --add channels bioconda\n",
    "conda config --add channels conda-forge\n",
    "\n",
    "conda create -n variant_calling\n",
    "conda activate variant_calling\n",
    "conda install nextflow=22.10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to set up the input parameters for the pipeline to run. Since we are using our own reference data, we first need to build index files for BWA (using BWA), a fasta index (using samtools), and a reference dictionary (using Picard Tools). These files will be located in the same directory as the reference fasta file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c bioconda bwa samtools picard\n",
    "\n",
    "# we perform the steps for sHP063 first\n",
    "cd sHP063/\n",
    "bwa index sHP063_reference.fasta\n",
    "samtools faidx sHP063_reference.fasta\n",
    "picard CreateSequenceDictionary R=sHP063_reference.fasta O=sHP063_reference.dict\n",
    "\n",
    "# and for sHP067\n",
    "cd ../sHP067/\n",
    "bwa index sHP067_reference.fasta\n",
    "samtools faidx sHP067_reference.fasta\n",
    "picard CreateSequenceDictionary R=sHP067_reference.fasta O=sHP067_reference.dict\n",
    "\n",
    "cd .. # back to main directory '../variant_calling/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to download snpEff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget -O snpEff_v4_3i_core.zip \"https://sourceforge.net/projects/snpeff/files/snpEff_v4_3i_core.zip/download\"\n",
    "unzip snpEff_v4_3i_core.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move the relevant scripts in our main directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv variant-calling-pipeline-gatk4/main.nf .\n",
    "mv variant-calling-pipeline-gatk4/bin/parse_metrics.sh ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And modify the ```main.nf``` script to specify the correct file path for ```snpEff``` and ```parse_metrics.sh```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the full path to snpEff.jar within process snpEff (line 352)\n",
    "java -jar /full/path/to/variant_calling/snpEff/snpEff.jar -v \n",
    "\n",
    "# add the full path to parse_metrics.sh within process qc (line 383)\n",
    "/full/path/to/variant_calling/parse_metrics.sh ${pair_id} > ${pair_id}_report.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we need to create the two ```.config``` files for B2 and R8 that specify all the parameters for the pipeline to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Required Parameters\n",
    "params.reads = \"/full/path/to/variant_calling/sHP063/*_{1,2}.fq.gz\"\n",
    "params.ref = \"/full/path/to/variant_calling/sHP063/sHP063_reference.fasta\"\n",
    "params.outdir = \"/full/path/to/variant_calling/sHP063/vc_out\"\n",
    "params.snpeff_db = \"Saccharomyces_cerevisiae\"\n",
    "params.pl = \"illumina\"\n",
    "params.pm = \"novaseq\"\n",
    "\n",
    "// Set the Nextflow Working Directory\n",
    "// By default this gets set to params.outdir + '/nextflow_work_dir'\n",
    "workDir = params.outdir + '/nextflow_work_dir'\n",
    "\n",
    "process {\n",
    "    executor = 'pbs'\n",
    "\n",
    "    clusterOptions = '-l walltime=32:00:00 -l select=1:ncpus=20:mem=100gb'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and create a similar file also for sHP067 / R8.\n",
    "\n",
    "All is set. We can now call the script ```main.nf``` for each strain as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nextflow run main.nf -c nf_sHP063.config -with-singularity gencorefacility/variant-calling-pipeline-gatk4\n",
    "\n",
    "nextflow run main.nf -c nf_sHP067.config -with-singularity gencorefacility/variant-calling-pipeline-gatk4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nextflow will take care of submitting the jobs to the cluster for you and run all the scripts. The results will be saved in the output directory specificed in the config file.\n",
    "\n",
    "On the Imperial HPC, it took around 1h / 1h30 for the whole script to run.\n",
    "\n",
    "While the whole pipeline did run without errors, the snpEff step is not completely correct since the reference we are using (sHP063) does not use the same nomenclature as the reference snpEff uses. To overcome this, we can build our own database and run again snpEff using it.\n",
    "\n",
    "We first need to create a .gff file for our de novo assembled genome. We can do it using [Liftoff](https://github.com/agshumate/Liftoff)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c bioconda liftoff\n",
    "\n",
    "liftoff -g ../GCF_000146045.2_R64_genomic.gff -o sHP063/sHP063_reference.gff -p 16 -copies \\\n",
    "sHP063/sHP063_reference.fasta ../GCF_000146045.2_R64_genomic.fna "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the snpEff database as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd snpEff\n",
    "mkdir data\n",
    "mkdir data/sHP063\n",
    "\n",
    "cp ../sHP063/sHP063_reference.gff data/sHP063_sc/genes.gff\n",
    "cp ../sHP063/sHP063_reference.fasta data/sHP063_sc/sequences.fa\n",
    "\n",
    "echo \"sHP063_sc.genome : sHP063_sc\" >> snpEff.config\n",
    "java -jar snpEff.jar build -gff3 -v sHP063_sc\n",
    "\n",
    "# double check the database has been added \n",
    "java -jar snpEff.jar databases | grep -i sHP063"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and re-run the snpEff annotation step as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../sHP063\n",
    "mkdir snpEff_out\n",
    "\n",
    "# SNPs\n",
    "java -jar ../snpEff/snpEff.jar sHP063_sc vc_out/out/filtered_snps/B2_filtered_snps_2.vcf > snpEff_out/B2_filtered_snps.ann.vcf\n",
    "\n",
    "# indels\n",
    "java -jar ../snpEff/snpEff.jar sHP063_sc vc_out/out/filtered_indels/B2_filtered_indels_2.vcf > snpEff_out/B2_filtered_indels.ann.vcf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally repeat the same steps for sHP067 and R8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda deactivate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 CNVs\n",
    "\n",
    "CNVs are a type of structural variation showing deletions or duplications in the genome. We will use [CNVpytor](https://github.com/abyzovlab/CNVpytor) to detect them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make new output folders to store the results of our analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir sHP063/cnv && mkdir sHP063/cnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a separate environment and install CNVpytor using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda create -n cnvpytor_env python=3.12 -y\n",
    "conda activate cnvpytor_env\n",
    "pip install git+https://github.com/abyzovlab/CNVpytor.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to configure our own reference genome, since CNVpytor installation includes only the human genomes hg19 and hg38. We can follow [these instructions](https://github.com/abyzovlab/CNVpytor/blob/master/examples/AddReferenceGenome.md):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd sHP063/cnv && cp ../sHP063_reference.fasta .\n",
    "cnvpytor -root sHP063_file.pytor -gc sHP063_reference.fasta -make_gc_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have a mask file so we will skip that step. \n",
    "\n",
    "Now let's create the ```ref_genome_conf.py```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_reference_genomes = {\n",
    "    \"sc_sHP063\": {\n",
    "        \"name\": \"sHP063\",\n",
    "        \"species\": \"Saccharomyces cerevisiae sHP063\",\n",
    "        \"chromosomes\": OrderedDict(\n",
    "            [('NC_001133.9_RagTag', (229557, 'A')), ('NC_001134.8_RagTag', (807497, 'A')), ('NC_001135.5_RagTag', (317092, 'A')), \n",
    "             ('NC_001136.10_RagTag', (1526506, 'A')), ('NC_001137.3_RagTag', (570770, 'A')), ('NC_001138.5_RagTag', (265709, 'A')), \n",
    "             ('NC_001139.9_RagTag', (1086801, 'A')), ('NC_001140.6_RagTag', (553953, 'A')), ('NC_001141.2_RagTag', (421153, 'A')), \n",
    "             ('NC_001142.9_RagTag', (728491, 'A')), ('NC_001143.9_RagTag', (667136, 'A')), ('NC_001144.5_RagTag', (1057334, 'A')), \n",
    "             ('NC_001145.3_RagTag', (920445, 'A')), ('NC_001146.8_RagTag', (776296, 'A')), ('NC_001147.6_RagTag', (1080639, 'A')), \n",
    "             ('NC_001148.4_RagTag', (928892, 'A')), ('NC_001224.1_RagTag', (7178, 'M'))]),\n",
    "        \"gc_file\":\"/full/path/to/variant_calling/sHP063/cnv/sHP063_file.pytor\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify the names and length of each chromosomes, you can use the following script in python (you will need the ```biopython``` package installed --> ```pip install biopython```):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "def process_fasta(file_path):\n",
    "    chromosomes = []\n",
    "    for record in SeqIO.parse(file_path, \"fasta\"):\n",
    "        chrom_name = record.id\n",
    "        chrom_length = len(record.seq)\n",
    "        tag = \"A\"\n",
    "        \n",
    "        chromosomes.append((chrom_name, (chrom_length, tag)))\n",
    "    \n",
    "    return chromosomes\n",
    "\n",
    "fasta_file = \"/path/to/variant_calling/sHP067/cnv/sHP063_reference.fasta\"  # Replace with your FASTA file path\n",
    "chromosome_data = process_fasta(fasta_file)\n",
    "print(chromosome_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove all the fragments (i.e. all chromosomes names not starting with NC_XXXXX_RagTag). Also we should modify the last chromosome type to mitochondrial - NC_001224.1_RagTag -> 'M'.\n",
    "\n",
    "The same steps need to be repeated for sHP067."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the analysis, we also need to index and sort our reads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install samtools bwa -y\n",
    "\n",
    "bwa index sHP063_reference.fasta \n",
    "bwa mem sHP063_reference.fasta ../B2_1.fq.gz ../B2_2.fq.gz > B2_alignment.sam # can take some time (up to 30min)\n",
    "samtools view -Sb B2_alignment.sam > B2_alignment.bam\n",
    "samtools sort B2_alignment.bam -o B2_sorted_alignment.bam\n",
    "samtools index B2_sorted_alignment.bam "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, you will need to repeat this for sHP067 / R8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now read to predict CNV regions. We can follow [this guide](https://github.com/abyzovlab/CNVpytor/wiki/2.-Read-Depth-Signal):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnvpytor -conf sHP063_ref_genome_conf.py -root B2.pytor \\\n",
    "-chrom NC_001133.9_RagTag NC_001134.8_RagTag NC_001135.5_RagTag NC_001136.10_RagTag NC_001137.3_RagTag NC_001138.5_RagTag NC_001139.9_RagTag NC_001140.6_RagTag NC_001141.2_RagTag NC_001142.9_RagTag NC_001143.9_RagTag NC_001144.5_RagTag NC_001145.3_RagTag NC_001146.8_RagTag NC_001147.6_RagTag NC_001148.4_RagTag NC_001224.1_RagTag \\\n",
    "-rd B2_sorted_alignment.bam \n",
    "\n",
    "# we can double-check we are using the right reference genome with\n",
    "cnvpytor -root B2.pytor -ls\n",
    "\n",
    "# calculate read depth histograms, GC correction and statistics type\n",
    "cnvpytor -conf sHP063_ref_genome_conf.py -root B2.pytor -his 100 1000 10000 100000\n",
    "\n",
    "# partitioning using mean-shift method\n",
    "cnvpytor -conf sHP063_ref_genome_conf.py -root B2.pytor -partition 100 1000 10000 100000\n",
    "\n",
    "# call CNV regions for each bin size\n",
    "cnvpytor -conf sHP063_ref_genome_conf.py -root B2.pytor -call 100 > B2.calls.100.tsv\n",
    "cnvpytor -conf sHP063_ref_genome_conf.py -root B2.pytor -call 1000 > B2.calls.1000.tsv\n",
    "cnvpytor -conf sHP063_ref_genome_conf.py -root B2.pytor -call 10000 > B2.calls.10000.tsv\n",
    "cnvpytor -conf sHP063_ref_genome_conf.py -root B2.pytor -call 100000 > B2.calls.100000.tsv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then repeat the same steps to identify CNVs in R8 with respect to sHP067."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda deactivate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
